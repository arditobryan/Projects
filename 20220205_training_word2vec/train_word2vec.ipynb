{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ardit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ardit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'today went supermarket buying grocieries'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence, stemming=False, lemmatizing=False):\n",
    "  #clean as much as possible, but not apply strong editing to the text, yet\n",
    "  sentence=str(sentence)\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "  sentence = sentence.lower()\n",
    "  sentence=sentence.replace('{html}',\"\") \n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', sentence)\n",
    "  rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "  rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "  tokens = tokenizer.tokenize(rem_num)\n",
    "  \n",
    "  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "  \n",
    "  if stemming == True and lemmatizing == False:\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    return \" \".join(stem_words)\n",
    "\n",
    "  if stemming == False and lemmatizing == True:\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "  if stemming == True and lemmatizing == True:\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(lemma_words)\n",
    "  \n",
    "  #at the end of the algo we return filtered words\n",
    "  return \" \".join(filtered_words)\n",
    "\n",
    "text = ['/Today I went /n to the supermarket /n for buying grocieries', \n",
    "        'if it works@@@ do not touch it!!!22']\n",
    "\n",
    "preprocess(text[0], stemming=False, lemmatizing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74940"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load list of sentences\n",
    "str1 = [x for x in open('bible.txt', errors='ignore').read().split('\\n') if x != '']\n",
    "len(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005577904"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "class MyCorpus:\n",
    "    def __init__(self, sentence_list):\n",
    "        self.sentence_list = sentence_list\n",
    "\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __iter__(self):\n",
    "        for sentence in self.sentence_list:\n",
    "            #ex. sentence = 'how are you today'\n",
    "            sentence = preprocess(sentence, stemming=False, lemmatizing=True)\n",
    "            yield utils.simple_preprocess(sentence) #tokenizer\n",
    "\n",
    "sentences = MyCorpus(str1)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=sentences)\n",
    "model.save('bible_freak.d2v')\n",
    "model\n",
    "\n",
    "vec_king = model.wv['people']\n",
    "vec_king[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x222896c8be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "pt_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "pt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "pt_model = models.KeyedVectors.load(\"glove-wiki-gigaword-300.d2v\")\n",
    "model = models.KeyedVectors.load(\"bible_freak.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', 0.6920520067214966),\n",
       " ('know', 0.65914386510849),\n",
       " ('something', 0.650749683380127),\n",
       " ('why', 0.6481607556343079),\n",
       " ('what', 0.6475381851196289),\n",
       " ('thing', 0.6450427174568176),\n",
       " ('unfortunately', 0.6372312307357788),\n",
       " ('mistake', 0.6358156204223633),\n",
       " ('anyway', 0.635249137878418),\n",
       " ('guess', 0.6345593333244324),\n",
       " ('anything', 0.6320500373840332),\n",
       " ('nothing', 0.631810188293457),\n",
       " ('really', 0.6262556910514832),\n",
       " ('believe', 0.6219422221183777),\n",
       " ('simply', 0.6162066459655762),\n",
       " ('thought', 0.6080223917961121),\n",
       " ('you', 0.6067540645599365),\n",
       " (\"n't\", 0.6059908270835876),\n",
       " ('say', 0.6048604846000671),\n",
       " ('sure', 0.6046518087387085),\n",
       " ('?', 0.6036630272865295),\n",
       " ('else', 0.6017588973045349),\n",
       " ('correct', 0.6011265516281128),\n",
       " ('exactly', 0.5988370180130005),\n",
       " ('fact', 0.598183810710907),\n",
       " ('going', 0.5973911285400391),\n",
       " ('actually', 0.5967668294906616),\n",
       " ('somebody', 0.5964211225509644),\n",
       " ('indeed', 0.5957838892936707),\n",
       " ('reason', 0.5936909914016724)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model.most_similar(positive=['wrong'], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ardit\\AppData\\Local\\Temp/ipykernel_23832/1778253366.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive=['bad'], topn=30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('given', 0.9998049139976501),\n",
       " ('good', 0.9998006820678711),\n",
       " ('rather', 0.9997993111610413),\n",
       " ('quality', 0.999788761138916),\n",
       " ('began', 0.9997869729995728),\n",
       " ('nature', 0.9997857213020325),\n",
       " ('found', 0.999785304069519),\n",
       " ('importance', 0.9997770190238953),\n",
       " ('say', 0.9997762441635132),\n",
       " ('vienna', 0.9997753500938416),\n",
       " ('ground', 0.9997749328613281),\n",
       " ('however', 0.9997736811637878),\n",
       " ('often', 0.9997730851173401),\n",
       " ('inner', 0.9997721910476685),\n",
       " ('level', 0.9997721910476685),\n",
       " ('effort', 0.999771237373352),\n",
       " ('jewish', 0.9997707605361938),\n",
       " ('especially', 0.9997705221176147),\n",
       " ('position', 0.9997694492340088),\n",
       " ('reason', 0.9997687935829163),\n",
       " ('period', 0.9997685551643372),\n",
       " ('development', 0.9997684955596924),\n",
       " ('situation', 0.9997682571411133),\n",
       " ('spirit', 0.9997681379318237),\n",
       " ('remained', 0.9997654557228088),\n",
       " ('intellectual', 0.9997624158859253),\n",
       " ('well', 0.9997622966766357),\n",
       " ('feeling', 0.9997620582580566),\n",
       " ('real', 0.9997615218162537),\n",
       " ('activity', 0.9997605681419373)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(list(model.wv.__dict__['vocab'].keys())))\n",
    "model.most_similar(positive=['bad'], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6324/1492500399.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ethical'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'difference'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'distance'"
     ]
    }
   ],
   "source": [
    "model.distance('ethical', 'difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = list()\n",
    "for pt_model_word in [x[0] for x in pt_model.most_similar(positive=['ethical'], topn=100)]:\n",
    "    #search for matches in model\n",
    "    if pt_model_word in sorted(list(model.wv.__dict__['vocab'].keys())):\n",
    "        matches.append(pt_model_word)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ardit\\AppData\\Local\\Temp/ipykernel_6324/1705742740.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive=['ethical'], topn=30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('difference', 0.9972132444381714),\n",
       " ('filled', 0.9971580505371094),\n",
       " ('fellow', 0.9971270561218262),\n",
       " ('equal', 0.997123122215271),\n",
       " ('reich', 0.9971152544021606),\n",
       " ('judgment', 0.9971123933792114),\n",
       " ('chief', 0.9970957636833191),\n",
       " ('original', 0.997094988822937),\n",
       " ('guard', 0.9970882534980774),\n",
       " ('happened', 0.997084379196167),\n",
       " ('organized', 0.9970834851264954),\n",
       " ('world', 0.9970805644989014),\n",
       " ('realize', 0.997078537940979),\n",
       " ('instinct', 0.9970767498016357),\n",
       " ('result', 0.997076153755188),\n",
       " ('view', 0.9970678091049194),\n",
       " ('grow', 0.9970676898956299),\n",
       " ('source', 0.9970670938491821),\n",
       " ('feeble', 0.9970472455024719),\n",
       " ('author', 0.9970463514328003),\n",
       " ('decisive', 0.9970443248748779),\n",
       " ('right', 0.9970428347587585),\n",
       " ('offensive', 0.9970405101776123),\n",
       " ('popular', 0.9970403909683228),\n",
       " ('mean', 0.9970374703407288),\n",
       " ('based', 0.9970347881317139),\n",
       " ('talent', 0.9970321655273438),\n",
       " ('evil', 0.9970312118530273),\n",
       " ('standard', 0.9970303773880005),\n",
       " ('book', 0.9970283508300781)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reference word\n",
    "model.most_similar(positive=['ethical'], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6324/4059382356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ethical'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'moral'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'distance'"
     ]
    }
   ],
   "source": [
    "model.distance('ethical', 'moral')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75a300ae82dd7b8f387c1777b66b2ec8c7a5f6d51d6392630ee9b10fab7f95f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
